# -*- coding: utf-8 -*-
"""M23CSA004_assign3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ggLn1toE6rZMyH42hvUhxgn8h3HIFYiW

Experiment 6.1
"""

import os
import torch
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim

# Open the image and mask files
image1 = Image.open("/content/drive/MyDrive/dlassign3/ISIC 2016/train/ISIC_0000002.jpg")

# Get the dimensions of the original image and mask
image_width, image_height = image1.size

# Print the dimensions
print("Original Image Dimensions: {} x {}".format(image_width, image_height))

# Open the image and mask files
image2 = Image.open("/content/drive/MyDrive/dlassign3/ISIC 2016/train/ISIC_0000100.jpg")

# Get the dimensions of the original image and mask
image_width, image_height = image2.size

# Print the dimensions
print("Original Image Dimensions: {} x {}".format(image_width, image_height))

import os
import torch
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchsummary import summary

# Define dataset class
class ISICDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.images_folder = os.path.join(root_dir, 'train')
        self.masks_folder = os.path.join(root_dir, 'train_masks')
        self.image_list = sorted(os.listdir(self.images_folder))
        self.mask_list = sorted(os.listdir(self.masks_folder))

    def __len__(self):
        return len(self.image_list)

    def __getitem__(self, idx):
        img_name = os.path.join(self.images_folder, self.image_list[idx])
        mask_name = os.path.join(self.masks_folder, self.image_list[idx].replace('.jpg', '.png'))
        image = Image.open(img_name).convert('RGB')
        mask = Image.open(mask_name).convert('L')

        if self.transform:
            image = self.transform(image)
            mask = self.transform(mask)

        return image, mask, self.image_list[idx], self.image_list[idx].replace('.jpg', '.png')

# Define data augmentation
train_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
])

# Define transforms for test data
test_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

# Define dataset and dataloader for training data
train_dataset = ISICDataset(root_dir='/content/drive/MyDrive/dlassign3/ISIC 2016', transform=train_transform)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Define dataset and dataloader for test data
test_dataset = ISICDataset(root_dir='/content/drive/MyDrive/dlassign3/ISIC 2016', transform=test_transform)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Display some data from train and train_mask folders
sample_images, sample_masks, image_filenames, mask_filenames = next(iter(train_dataloader))

# Print final size of train, train_mask
print("Initial size and shape of train data after resize:")
print(sample_images.size())
print("Initial size and shape of train mask data after resize:")
print(sample_masks.size())

# Print final size of test, test_mask
sample_images1, sample_masks1, image_filenames1, mask_filenames1 = next(iter(test_dataloader))
print("Initial size and shape of test data after resize:")
print(sample_images1.size())
print("Initial size and shape of test mask data after resize:")
print(sample_masks1.size())


# Display some sample images and masks with filenames
fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))
for i in range(4):
    axes[0, i].imshow(sample_images[i].permute(1, 2, 0))
    axes[0, i].axis('off')
    axes[0, i].set_title('Image\n' + image_filenames[i])
    axes[1, i].imshow(sample_masks[i][0], cmap='gray')
    axes[1, i].axis('off')
    axes[1, i].set_title('Mask\n' + mask_filenames[i])
plt.tight_layout()
plt.show()

# Load Pre-trained MobileNet Encoder
encoder = models.mobilenet_v2(pretrained=True)

# Freeze encoder parameters
for param in encoder.parameters():
    param.requires_grad = False

class CustomDecoder(nn.Module):
    def __init__(self, input_channels, num_classes):
        super(CustomDecoder, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 128, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(128, num_classes, kernel_size=1)
        self.upsample = nn.Upsample(size=(128,128),mode='bilinear')

    def forward(self, x):
        #print(x.shape)
        x = F.relu(self.conv1(x))
        #x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)  # Upsample to match input size
        x = self.conv2(x)
        x = self.upsample(x)
        #print(x.shape)
        return x


# Define Segmentation Model
input_channels = 1280  # Number of output channels from the MobileNet encoder
num_classes = 1  # Number of classes in the segmentation task (binary segmentation)
decoder = CustomDecoder(input_channels, num_classes)
segmentation_model = nn.Sequential(encoder.features, decoder)

# Print model summaries
#print("Encoder Summary:")
#summary(encoder, (3, 128, 128))
#print("\nDecoder Summary:")
#summary(decoder, (input_channels, 8, 8))

# Define loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(segmentation_model.parameters(), lr=0.001)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
segmentation_model.to(device)

# Training loop
train_loss_history = []
val_loss_history = []
epochs = 10

# Training loop
for epoch in range(epochs):
    train_loss = 0.0
    val_loss = 0.0

    # Training loop
    segmentation_model.train()
    for images, masks, _, _ in train_dataloader:
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad()
        outputs = segmentation_model(images)

        # Resize target masks to match the size of model outputs
        #target_masks_resized = F.interpolate(masks, size=(32, 32), mode='bilinear', align_corners=True)

        # Calculate loss using resized target masks
        #loss = criterion(outputs, target_masks_resized)
        loss = criterion(outputs, masks)

        loss.backward()
        optimizer.step()
        train_loss += loss.item() * images.size(0)


    # Validation loop
    segmentation_model.eval()
    with torch.no_grad():
        for images, masks, _, _ in test_dataloader:  # Ignore the last two values
            images, masks = images.to(device), masks.to(device)
            outputs = segmentation_model(images)

            # Resize target masks to match the size of model outputs
            #target_masks_resized = F.interpolate(masks, size=(32, 32), mode='bilinear', align_corners=True)

            # Calculate loss using resized target masks
            loss = criterion(outputs, masks)

            val_loss += loss.item() * images.size(0)

    # Append loss values to history for visualization
    train_loss1 = train_loss / len(train_dataloader.dataset)
    train_loss_history.append(train_loss1)
    val_loss1 = val_loss / len(test_dataloader.dataset)
    val_loss_history.append(val_loss1)


    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss1:.4f}, Val Loss: {val_loss1:.4f}')

# Plot the training and validation loss
plt.plot(train_loss_history, label='Training Loss')
plt.plot(val_loss_history, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Calculate and report IoU and Dice Score
def calculate_iou(pred_mask, true_mask):
    intersection = torch.logical_and(pred_mask, true_mask).sum()
    union = torch.logical_or(pred_mask, true_mask).sum()
    iou = intersection.item() / union.item()
    return iou

def calculate_dice(pred_mask, true_mask):
    intersection = torch.logical_and(pred_mask, true_mask).sum()
    dice = (2. * intersection.item()) / (pred_mask.sum().item() + true_mask.sum().item())
    return dice

# find the iou and dice_scores
iou_scores = []
dice_scores = []
segmentation_model.eval()
with torch.no_grad():
    for images, true_masks, _, _ in test_dataloader:
        images, true_masks = images.to(device), true_masks.to(device)
        outputs = segmentation_model(images)
        pred_masks = torch.sigmoid(outputs) > 0.5
        for i in range(len(images)):
            iou = calculate_iou(pred_masks[i], true_masks[i])
            dice = calculate_dice(pred_masks[i], true_masks[i])
            iou_scores.append(iou)
            dice_scores.append(dice)

avg_iou = sum(iou_scores) / len(iou_scores)
avg_dice = sum(dice_scores) / len(dice_scores)
print(f'Average IoU: {avg_iou:.4f}')
print(f'Average Dice Score: {avg_dice:.4f}')

# # Visualize samples with masks and ground truth
# def visualize_samples(model, dataloader, num_samples=5):
#     model.eval()
#     with torch.no_grad():
#         for i, (images, true_masks, _, _) in enumerate(dataloader):  # Ignore the last two values
#             if i == num_samples:
#                 break
#             # Process the images and masks (to device, forward pass, etc.)
#             images, true_masks = images.to(device), true_masks.to(device)
#             outputs = model(images)
#             pred_masks = torch.sigmoid(outputs) > 0.5

#             # Plot the images, ground truth masks, and predicted masks
#             plt.figure(figsize=(12, 6))
#             for j in range(len(images)):
#                 plt.subplot(3, num_samples, j + 1)
#                 plt.imshow(images[j].permute(1, 2, 0))
#                 plt.title('Image')
#                 plt.axis('off')

#                 plt.subplot(3, num_samples, num_samples + j + 1)
#                 plt.imshow(true_masks[j][0], cmap='gray')
#                 plt.title('Ground Truth Mask')
#                 plt.axis('off')

#                 plt.subplot(3, num_samples, 2*num_samples + j + 1)
#                 plt.imshow(pred_masks[j][0], cmap='gray')
#                 plt.title('Predicted Mask')
#                 plt.axis('off')
#             plt.tight_layout()
#             plt.show()

# # Visualize samples alongside their generated masks and ground truth
# visualize_samples(segmentation_model, test_dataloader)

"""EXperiment 6.2"""

import os
import torch
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchsummary import summary  # For model summary

# Define dataset class
class ISICDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.images_folder = os.path.join(root_dir, 'train')
        self.masks_folder = os.path.join(root_dir, 'train_masks')
        self.image_list = sorted(os.listdir(self.images_folder))
        self.mask_list = sorted(os.listdir(self.masks_folder))

    def __len__(self):
        return len(self.image_list)

    def __getitem__(self, idx):
        img_name = os.path.join(self.images_folder, self.image_list[idx])
        mask_name = os.path.join(self.masks_folder, self.image_list[idx].replace('.jpg', '.png'))
        image = Image.open(img_name).convert('RGB')
        mask = Image.open(mask_name).convert('L')

        if self.transform:
            image = self.transform(image)
            mask = self.transform(mask)

        return image, mask, self.image_list[idx], self.image_list[idx].replace('.jpg', '.png')

# Define data augmentation
train_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
])

# Define transforms for test data
test_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

# Define dataset and dataloader for training data
train_dataset = ISICDataset(root_dir='/content/drive/MyDrive/dlassign3/ISIC 2016', transform=train_transform)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Define dataset and dataloader for test data
test_dataset = ISICDataset(root_dir='/content/drive/MyDrive/dlassign3/ISIC 2016', transform=test_transform)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Display some data from train and train_mask folders
sample_images, sample_masks, image_filenames, mask_filenames = next(iter(train_dataloader))

# Print final size of train, train_mask
print("Initial size and shape of train data after resize:")
print(sample_images.size())
print("Initial size and shape of train mask data after resize:")
print(sample_masks.size())

# Print final size of test, test_mask
sample_images1, sample_masks1, image_filenames1, mask_filenames1 = next(iter(test_dataloader))
print("Initial size and shape of test data after resize:")
print(sample_images1.size())
print("Initial size and shape of test mask data after resize:")
print(sample_masks1.size())


# Display some sample images and masks with filenames
fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))
for i in range(4):
    axes[0, i].imshow(sample_images[i].permute(1, 2, 0))
    axes[0, i].axis('off')
    axes[0, i].set_title('Image\n' + image_filenames[i])
    axes[1, i].imshow(sample_masks[i][0], cmap='gray')
    axes[1, i].axis('off')
    axes[1, i].set_title('Mask\n' + mask_filenames[i])
plt.tight_layout()
plt.show()

# Load Pre-trained MobileNet Encoder
encoder_2 = models.mobilenet_v2(pretrained=True)


class CustomDecoder_2(nn.Module):
    def __init__(self, input_channels, num_classes):
        super(CustomDecoder_2, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 128, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(128, num_classes, kernel_size=1)
        self.upsample = nn.Upsample(size=(128,128),mode='bilinear')

    def forward(self, x):
        #print(x.shape)
        x = F.relu(self.conv1(x))
        #x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)  # Upsample to match input size
        x = self.conv2(x)
        x = self.upsample(x)
        #print(x.shape)
        return x


# Define Segmentation Model
input_channels = 1280  # Number of output channels from the MobileNet encoder
num_classes = 1  # Number of classes in the segmentation task (binary segmentation)
decoder_2 = CustomDecoder_2(input_channels, num_classes)
segmentation_model_1 = nn.Sequential(encoder_2.features, decoder_2)

# Print model summaries
#print("Encoder Summary:")
#summary(encoder, (3, 128, 128))  # Assuming input size is 128x128 and 3 channels (RGB)
#print("\nDecoder Summary:")
#summary(decoder, (input_channels, 8, 8))  # Assuming input size is 8x8 and input channels are from the encoder

# Define loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(segmentation_model_1.parameters(), lr=0.001)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
segmentation_model_1.to(device)

# Training loop
train_loss_history = []
val_loss_history = []
epochs = 10

# Training loop
for epoch in range(epochs):
    train_loss = 0.0
    val_loss = 0.0

    # Training loop
    segmentation_model_1.train()
    for images, masks, _, _ in train_dataloader:
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad()
        outputs = segmentation_model_1(images)

        # Resize target masks to match the size of model outputs
        #target_masks_resized = F.interpolate(masks, size=(32, 32), mode='bilinear', align_corners=True)

        # Calculate loss using resized target masks
        #loss = criterion(outputs, target_masks_resized)
        loss = criterion(outputs, masks)

        loss.backward()
        optimizer.step()
        train_loss += loss.item() * images.size(0)


    # Validation loop
    segmentation_model_1.eval()
    with torch.no_grad():
        for images, masks, _, _ in test_dataloader:
            images, masks = images.to(device), masks.to(device)
            outputs = segmentation_model_1(images)

            # Resize target masks to match the size of model outputs
            #target_masks_resized = F.interpolate(masks, size=(32, 32), mode='bilinear', align_corners=True)

            # Calculate loss using resized target masks
            loss = criterion(outputs, masks)

            val_loss += loss.item() * images.size(0)

    # Append loss values to history for visualization
    train_loss1 = train_loss / len(train_dataloader.dataset)
    train_loss_history.append(train_loss1)
    val_loss1 = val_loss / len(test_dataloader.dataset)
    val_loss_history.append(val_loss1)


    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss1:.4f}, Val Loss: {val_loss1:.4f}')

# Plot the training and validation loss
plt.plot(train_loss_history, label='Training Loss')
plt.plot(val_loss_history, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Calculate and report IoU and Dice Score
def calculate_iou(pred_mask, true_mask):
    intersection = torch.logical_and(pred_mask, true_mask).sum()
    union = torch.logical_or(pred_mask, true_mask).sum()
    iou = intersection.item() / union.item()
    return iou

def calculate_dice(pred_mask, true_mask):
    intersection = torch.logical_and(pred_mask, true_mask).sum()
    dice = (2. * intersection.item()) / (pred_mask.sum().item() + true_mask.sum().item())
    return dice


iou_scores = []
dice_scores = []
segmentation_model_1.eval()
with torch.no_grad():
    for images, true_masks, _, _ in test_dataloader:
        images, true_masks = images.to(device), true_masks.to(device)
        outputs = segmentation_model_1(images)
        pred_masks = torch.sigmoid(outputs) > 0.5
        for i in range(len(images)):
            iou = calculate_iou(pred_masks[i], true_masks[i])
            dice = calculate_dice(pred_masks[i], true_masks[i])
            iou_scores.append(iou)
            dice_scores.append(dice)

avg_iou = sum(iou_scores) / len(iou_scores)
avg_dice = sum(dice_scores) / len(dice_scores)
print(f'Average IoU: {avg_iou:.4f}')
print(f'Average Dice Score: {avg_dice:.4f}')

def visualize_samples(model, dataloader, num_samples=5):
    model.eval()
    with torch.no_grad():
        for i, (images, true_masks, _, _) in enumerate(dataloader):
            if i >= num_samples:
                break
            images, true_masks = images.to(device), true_masks.to(device)
            outputs = model(images)
            pred_masks = torch.sigmoid(outputs) > 0.5

            plt.figure(figsize=(12, 6))
            for j in range(len(images)):
                plt.subplot(1, 3, 1)
                plt.imshow(images[j].permute(1, 2, 0))
                plt.title('Image')
                plt.axis('off')

                plt.subplot(1, 3, 2)
                plt.imshow(true_masks[j][0], cmap='gray')
                plt.title('Ground Truth Mask')
                plt.axis('off')

                plt.subplot(1, 3, 3)
                plt.imshow(pred_masks[j][0], cmap='gray')
                plt.title('Predicted Mask')
                plt.axis('off')
            plt.tight_layout()
            plt.show()

# Visualize a single sample alongside its generated mask and ground truth
visualize_samples(segmentation_model_1, test_dataloader, num_samples=5)

























